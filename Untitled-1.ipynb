{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11c0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "در حال خواندن و ادغام فایل‌ها با ستون‌های مشخص شده...\n",
      "✅ فایل anger.csv با 20069 رکورد بارگذاری شد. احساس: anger\n",
      "✅ فایل disgust.csv با 925 رکورد بارگذاری شد. احساس: disgust\n",
      "✅ فایل fear.csv با 17624 رکورد بارگذاری شد. احساس: fear\n",
      "✅ فایل joy.csv با 28024 رکورد بارگذاری شد. احساس: joy\n",
      "✅ فایل sad.csv با 34328 رکورد بارگذاری شد. احساس: sad\n",
      "✅ فایل surprise.csv با 12859 رکورد بارگذاری شد. احساس: surprise\n",
      "\n",
      "------------------------------------\n",
      "✅ ادغام با موفقیت انجام شد. تعداد کل رکوردها: 113829\n",
      "\n",
      "۵ سطر تصادفی از مجموعه داده نهایی:\n",
      "                                             Text_Content Sentiment_Label\n",
      "106222  بهاره جان تیتر بزن مخازن آمونیاک هم سرگردان شد...        surprise\n",
      "73337    اگر میخواست میگفت رای بدید! #دروغ_ممنوع \\nاین...             sad\n",
      "50737   مولای بزرگ ما، ارباب خوب خوبی‌ها، آزاده‌ی از ه...             joy\n",
      "20100     تا حالا تو آینه به خودت نگاه کردی و اون چهره...         disgust\n",
      "50393   امشب داشتم ترانه سرزمین من از داود سرخوش خوانن...             joy\n",
      "\n",
      "توزیع احساسات:\n",
      "Sentiment_Label\n",
      "sad         34328\n",
      "joy         28024\n",
      "anger       20069\n",
      "fear        17624\n",
      "surprise    12859\n",
      "disgust       925\n",
      "Name: count, dtype: int64\n",
      "\n",
      "تعداد رکوردهای تکراری حذف شده: 310\n",
      "\n",
      "✅ داده‌های ادغام شده در فایل 'persian_tweets_merged.csv' ذخیره شدند.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# نام فایل‌های CSV \n",
    "file_names = {\n",
    "    'anger': 'anger.csv',\n",
    "    'disgust': 'disgust.csv',\n",
    "    'fear': 'fear.csv',\n",
    "    'joy': 'joy.csv',\n",
    "    'sad': 'sad.csv',\n",
    "    'surprise': 'surprise.csv'\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "print(\"در حال خواندن و ادغام فایل‌ها با ستون‌های مشخص شده...\")\n",
    "\n",
    "for sentiment, file_name in file_names.items():\n",
    "    try:\n",
    "        # 1. خواندن فایل با فرض وجود هدر (header=0)\n",
    "        df_temp = pd.read_csv(file_name, encoding='utf-8')\n",
    "        \n",
    "        # 2. انتخاب ستون‌های اصلی و تغییر نام ستون متن برای یکپارچگی\n",
    "        # ما فقط ستون‌های 'tweet' و 'emotion' را نیاز داریم.\n",
    "        df_temp = df_temp[['tweet', 'emotion']]\n",
    "        \n",
    "        # نکته: چون هر فایل فقط شامل یک نوع احساس (بر اساس نام فایل) است، \n",
    "        # ستون 'emotion' در واقع فقط یک مقدار ثابت (مثل 'anger') خواهد داشت.\n",
    "        # ما از ستون 'emotion' اصلی استفاده می کنیم که در دیتاست وجود دارد.\n",
    "\n",
    "        df_temp = df_temp.rename(columns={'tweet': 'Text_Content', 'emotion': 'Sentiment_Label'})\n",
    "        \n",
    "        all_data.append(df_temp)\n",
    "        print(f\"✅ فایل {file_name} با {len(df_temp)} رکورد بارگذاری شد. احساس: {sentiment}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ خطا: فایل {file_name} پیدا نشد. مطمئن شوید که تمام شش فایل در مسیر درست هستند.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ خطایی در خواندن فایل {file_name} رخ داد: {e}\")\n",
    "\n",
    "# ادغام تمام DataFrameها به یک DataFrame نهایی\n",
    "if all_data:\n",
    "    df_merged = pd.concat(all_data, ignore_index=True)\n",
    "    print(\"\\n------------------------------------\")\n",
    "    print(f\"✅ ادغام با موفقیت انجام شد. تعداد کل رکوردها: {len(df_merged)}\")\n",
    "    \n",
    "    # نمایش ۵ سطر تصادفی از مجموعه داده نهایی\n",
    "    print(\"\\n۵ سطر تصادفی از مجموعه داده نهایی:\")\n",
    "    print(df_merged.sample(5))\n",
    "    \n",
    "    # نمایش توزیع برچسب‌ها\n",
    "    print(\"\\nتوزیع احساسات:\")\n",
    "    print(df_merged['Sentiment_Label'].value_counts())\n",
    "    \n",
    "    # حذف ردیف‌های تکراری احتمالی\n",
    "    initial_count = len(df_merged)\n",
    "    df_merged.drop_duplicates(subset=['Text_Content', 'Sentiment_Label'], inplace=True)\n",
    "    print(f\"\\nتعداد رکوردهای تکراری حذف شده: {initial_count - len(df_merged)}\")\n",
    "else:\n",
    "    print(\"هیچ فایلی برای ادغام وجود نداشت.\")\n",
    "\n",
    "# ذخیره سازی DataFrame ادغام شده در یک فایل جدید\n",
    "output_file_path = 'persian_tweets_merged.csv'\n",
    "df_merged.to_csv(output_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n✅ داده‌های ادغام شده در فایل '{output_file_path}' ذخیره شدند.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03f956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------\n",
      "✅ ستون Clean_Text ایجاد شد.\n",
      "\n",
      "نمونه‌ای از تمیزکاری:\n",
      "متن اصلی: اگر واقعاً همان مردمی هستید که در روز تشییع شهید #حاج_ قاسم سلیمانی به او قول دادید انتقام سختی می گیرید روز ۲۸ خرداد ۱۴۰۰ روز #انتقام_ سخت است!\n",
      "#به_عشق_امام_رضا https://t.co/HdBvayzX9W\n",
      "متن تمیز شده: اگر واقعا همان مردمی هستید که در روز تشییع شهید قاسم سلیمانی به او قول دادید انتقام سختی می‌گیرید روز ۲۸ خرداد ۱۴۰۰ روز سخت است\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from hazm import Normalizer\n",
    "\n",
    "# ۱. تعریف نرمالایزر Hazm\n",
    "normalizer = Normalizer()\n",
    "\n",
    "def clean_and_normalize(text):\n",
    "    # الف) تبدیل به رشته (در صورت نیاز)\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    # ب) حذف لینک‌ها\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # پ) حذف منشن‌ها (@username) و هشتگ‌ها (#tag)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # ت) حذف علائم نگارشی و کاراکترهای اضافی غیر از حروف و اعداد\n",
    "    text = re.sub(r'[^\\w\\s\\u0600-\\u06FF]', ' ', text) \n",
    "    \n",
    "    # ث) حذف فاصله‌های اضافی\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # ج) نرمال‌سازی با Hazm\n",
    "    text = normalizer.normalize(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# اعمال تابع تمیزکاری و نرمال‌سازی روی ستون متن ادغام شده\n",
    "# توجه: اگر کد ۱.۵ را اجرا کرده باشید، df_merged در حافظه موجود است.\n",
    "df_merged['Clean_Text'] = df_merged['Text_Content'].apply(clean_and_normalize)\n",
    "\n",
    "# نمایش نتایج برای اطمینان\n",
    "print(\"\\n------------------------------------\")\n",
    "print(\"✅ ستون Clean_Text ایجاد شد.\")\n",
    "print(\"\\nنمونه‌ای از تمیزکاری:\")\n",
    "sample_row = df_merged.sample(1)\n",
    "print(f\"متن اصلی: {sample_row['Text_Content'].iloc[0]}\")\n",
    "print(f\"متن تمیز شده: {sample_row['Clean_Text'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ae6ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ داده‌های تمیز شده و آماده مدل‌سازی در فایل 'persian_tweets_cleaned_for_model.csv' با اِنکدینگ سازگار ذخیره شدند.\n",
      "حالا می توانیم وارد فاز مدل‌سازی شویم.\n"
     ]
    }
   ],
   "source": [
    "# اجرای مجدد تمیزکاری برای اطمینان از وجود ستون Clean_Text\n",
    "# فرض می‌کنیم تمیزکاری در سلول قبل اجرا شده است.\n",
    "\n",
    "# ذخیره سازی نهایی DataFrame با ستون‌های تمیز شده\n",
    "final_output_file = 'persian_tweets_cleaned_for_model.csv'\n",
    "\n",
    "# index=False: برای حذف ستون‌های اضافی ایندکس\n",
    "# encoding='utf-8-sig': این همان UTF-8 with BOM است که برای سازگاری با Excel بهتر است.\n",
    "df_merged.to_csv(final_output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ داده‌های تمیز شده و آماده مدل‌سازی در فایل '{final_output_file}' با اِنکدینگ سازگار ذخیره شدند.\")\n",
    "print(\"حالا می توانیم وارد فاز مدل‌سازی شویم.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aa9fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ داده‌ها با موفقیت بارگذاری شدند.\n",
      "\n",
      "ستون‌های فعلی قبل از حذف:\n",
      "['Text_Content', 'Sentiment_Label', 'Clean_Text']\n",
      "✅ ستون‌های اضافی حذف شدند. DataFrame به 2 ستون کاهش یافت.\n",
      "\n",
      "✅ مجموعه داده بهینه شده در فایل 'persian_tweets_optimized_for_training.csv' ذخیره شد.\n",
      "این فایل فقط شامل ستون‌های Clean_Text و Sentiment_Label است.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# نام فایل نهایی که قبلاً ساختیم\n",
    "file_path_cleaned = 'persian_tweets_cleaned_for_model.csv'\n",
    "\n",
    "# ۱. بارگذاری فایل (با اطمینان از اِنکدینگ)\n",
    "try:\n",
    "    df_final = pd.read_csv(file_path_cleaned, encoding='utf-8-sig')\n",
    "    print(\"✅ داده‌ها با موفقیت بارگذاری شدند.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ خطایی در بارگذاری فایل رخ داد: {e}\")\n",
    "    # اگر در بارگذاری مشکل دارید، فقط encoding='utf-8' را امتحان کنید.\n",
    "    df_final = pd.read_csv(file_path_cleaned, encoding='utf-8')\n",
    "\n",
    "\n",
    "# ۲. نمایش ستون‌های فعلی برای تأیید\n",
    "print(\"\\nستون‌های فعلی قبل از حذف:\")\n",
    "print(df_final.columns.tolist())\n",
    "\n",
    "# ۳. حذف ستون Text_Content (و هر ستون اضافی دیگری که نیاز نیست)\n",
    "# ما فقط ستون‌های Clean_Text و Sentiment_Label را نگه می‌داریم.\n",
    "columns_to_keep = ['Clean_Text', 'Sentiment_Label']\n",
    "df_optimized = df_final[columns_to_keep]\n",
    "\n",
    "print(f\"✅ ستون‌های اضافی حذف شدند. DataFrame به {len(df_optimized.columns)} ستون کاهش یافت.\")\n",
    "\n",
    "# ۴. ذخیره سازی نهایی و بهینه سازی شده\n",
    "optimized_file_path = 'persian_tweets_optimized_for_training.csv'\n",
    "df_optimized.to_csv(optimized_file_path, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"\\n✅ مجموعه داده بهینه شده در فایل '{optimized_file_path}' ذخیره شد.\")\n",
    "print(\"این فایل فقط شامل ستون‌های Clean_Text و Sentiment_Label است.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
